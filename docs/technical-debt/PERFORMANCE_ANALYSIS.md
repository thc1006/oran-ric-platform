# O-RAN RIC Platform æ•ˆèƒ½å…¨é¢åˆ†æå ±å‘Š

**ä½œè€…ï¼š** è”¡ç§€å‰ (thc1006)
**æ—¥æœŸï¼š** 2025-11-17
**åˆ†æç¯„åœï¼š** O-RAN Near-RT RIC Platform J Release
**æ¸¬è©¦ç’°å¢ƒï¼š** k3s v1.28.5+k3s1 on Debian GNU/Linux 13

---

## åŸ·è¡Œæ‘˜è¦ (Executive Summary)

æœ¬å ±å‘Šé‡å° O-RAN RIC Platform é€²è¡Œå…¨é¢çš„æ•ˆèƒ½åˆ†æï¼Œæ¶µè“‹è³‡æºé…ç½®ã€ç³»çµ±æ•ˆèƒ½ã€æ“´å±•æ€§ä»¥åŠå„ªåŒ–æ©Ÿæœƒã€‚é€éåˆ†æç¾æœ‰é…ç½®ã€å¯¦éš›éƒ¨ç½²è³‡æ–™èˆ‡æ•ˆèƒ½æŒ‡æ¨™ï¼Œè­˜åˆ¥å‡ºå¤šå€‹é—œéµç“¶é ¸ä¸¦æå‡ºå…·é«”çš„å„ªåŒ–å»ºè­°ã€‚

### é—œéµç™¼ç¾ (Key Findings)

**ğŸ”´ åš´é‡å•é¡Œ (Critical Issues):**
1. **è³‡æºé…ç½®ä¸ä¸€è‡´** - Platform components èˆ‡ xApps ä¹‹é–“çš„è³‡æºé…ç½®ç­–ç•¥ä¸çµ±ä¸€
2. **ç¼ºä¹ HPA é…ç½®** - æ‰€æœ‰çµ„ä»¶éƒ½æœªå•Ÿç”¨ Horizontal Pod Autoscaling
3. **Prometheus è³‡æ–™ç•™å­˜ç­–ç•¥** - åƒ…ä¿ç•™ 15 å¤©ï¼Œå°é•·æœŸæ•ˆèƒ½åˆ†æä¸è¶³
4. **ç„¡ Resource Quotas** - Namespace å±¤ç´šæœªè¨­å®šè³‡æºé…é¡ï¼Œå­˜åœ¨è³‡æºè€—ç›¡é¢¨éšª

**ğŸŸ¡ é«˜å„ªå…ˆç´šå•é¡Œ (High Priority Issues):**
1. **RMR è¨Šæ¯è™•ç†æ•ˆèƒ½** - å–®ä¸€ worker é…ç½®å¯èƒ½æˆç‚ºç“¶é ¸
2. **Redis å–®é»æ•…éšœ** - DBaaS æœªå•Ÿç”¨ HA æ¨¡å¼
3. **ç¼ºä¹åˆ†æ•£å¼è¿½è¹¤** - Jaeger adapter æœªéƒ¨ç½²ï¼Œé›£ä»¥è¨ºæ–·å»¶é²å•é¡Œ
4. **Metrics æ¡é›†é–“éš”éé•·** - Prometheus scrape interval ç‚º 1 åˆ†é˜ï¼Œç„¡æ³•æ•æ‰çŸ­æœŸæ•ˆèƒ½å•é¡Œ

**ğŸŸ¢ å„ªåŒ–æ©Ÿæœƒ (Optimization Opportunities):**
1. å¯¦ä½œå¤šå±¤å¿«å–ç­–ç•¥ (Multi-tier Caching)
2. å„ªåŒ– E2 è¨Šæ¯è™•ç†æµç¨‹
3. å¯¦ä½œé€£ç·šæ± èˆ‡æ‰¹æ¬¡è™•ç†
4. å•Ÿç”¨ Service Mesh ä»¥æ”¹å–„ observability

---

## 1. è³‡æºé…ç½®åˆ†æ (Resource Allocation Analysis)

### 1.1 Platform Components è³‡æºé…ç½®

#### ç¾æ³åˆ†æ (Current State)

| Component | CPU Request | CPU Limit | Memory Request | Memory Limit | QoS Class |
|-----------|-------------|-----------|----------------|--------------|-----------|
| e2term | 400m | 1000m | 512Mi | 1Gi | Burstable |
| e2mgr | 200m | 500m | 256Mi | 512Mi | Burstable |
| submgr | 100m | 250m | 128Mi | 256Mi | Burstable |
| a1mediator | 100m | 250m | 128Mi | 256Mi | Burstable |
| appmgr | 200m | 500m | 256Mi | 512Mi | Burstable |
| dbaas (Redis) | 100m | 500m | 256Mi | 1Gi | Burstable |
| Prometheus | 500m | 1000m | 1Gi | 2Gi | Burstable |
| Grafana | 250m | 500m | 256Mi | 512Mi | Burstable |

**åˆ†æç™¼ç¾:**

1. **Request/Limit æ¯”ä¾‹ä¸åˆç†**
   - E2term: CPU limit æ˜¯ request çš„ 2.5 å€ (1000m/400m)
   - DBaaS: CPU limit æ˜¯ request çš„ 5 å€ (500m/100m)
   - å•é¡Œï¼šéé«˜çš„ limit æœƒå°è‡´ CPU throttlingï¼Œå½±éŸ¿å»¶é²æ•æ„Ÿçš„æ‡‰ç”¨

2. **æ‰€æœ‰çµ„ä»¶éƒ½æ˜¯ Burstable QoS**
   - ç„¡ Guaranteed QoS pods
   - åœ¨è³‡æºå£“åŠ›ä¸‹ï¼Œæ‰€æœ‰ pods éƒ½å¯èƒ½è¢«é©…é€
   - E2term ç­‰é—œéµçµ„ä»¶æ‡‰è©²ä½¿ç”¨ Guaranteed QoS

3. **ç¼ºä¹å¯¦éš›ä½¿ç”¨è³‡æ–™é©—è­‰**
   ```
   ç•¶å‰ç¯€é»è³‡æºä½¿ç”¨ç‡ï¼š
   - CPU: 1277m / 32000m (3.99%)
   - Memory: 7195Mi / 48000Mi (14.98%)
   ```
   - æ˜é¡¯è³‡æºæœªå……åˆ†åˆ©ç”¨
   - Requests å¯èƒ½è¨­å®šéæ–¼ä¿å®ˆ

### 1.2 xApps è³‡æºé…ç½®

#### ç¾æ³åˆ†æ

| xApp | CPU Request | CPU Limit | Memory Request | Memory Limit | Purpose |
|------|-------------|-----------|----------------|--------------|---------|
| kpimon | 200m | 1000m | 512Mi | 1Gi | KPI monitoring |
| traffic-steering | 200m | 500m | 256Mi | 512Mi | Handover decisions |
| rc-xapp | 200m | 500m | 256Mi | 512Mi | RAN control |
| qoe-predictor | 200m | 500m | 256Mi | 512Mi | QoE prediction |
| federated-learning | 200m | 500m | 256Mi | 512Mi | FL training |

**åˆ†æç™¼ç¾:**

1. **è³‡æºé…ç½®éæ–¼çµ±ä¸€**
   - é™¤äº† kpimonï¼Œå…¶ä»– xApps éƒ½ä½¿ç”¨ç›¸åŒçš„è³‡æºé…ç½®
   - æœªæ ¹æ“šå¯¦éš›å·¥ä½œè² è¼‰ç‰¹æ€§èª¿æ•´
   - Traffic-steering (æ§åˆ¶é¢) èˆ‡ kpimon (ç›£æ§é¢) æ‡‰æœ‰ä¸åŒçš„è³‡æºç­–ç•¥

2. **RMR Workers é…ç½®ä¸è¶³**
   ```json
   kpimon: "numWorkers": 2
   traffic-steering: "numWorkers": 1
   ```
   - Traffic-steering åªæœ‰ 1 å€‹ workerï¼Œè™•ç† E2 indication + A1 policy
   - å¯èƒ½æˆç‚ºå»¶é²ç“¶é ¸

3. **ç¼ºä¹ GPU è³‡æºç®¡ç†**
   - Federated-learning GPU pod è«‹æ±‚ `nvidia.com/gpu: 1`
   - ä½†ç„¡ fallback æ©Ÿåˆ¶æˆ–å„ªé›…é™ç´š

### 1.3 è³‡æºé…é¡ (Resource Quotas)

**å•é¡Œï¼šå®Œå…¨ç¼ºä¹ Namespace å±¤ç´šçš„è³‡æºé…é¡æ§åˆ¶**

```yaml
# ç•¶å‰ç‹€æ…‹ï¼šç„¡ ResourceQuota
kubectl get resourcequota -n ricplt
# No resources found

kubectl get resourcequota -n ricxapp
# No resources found
```

**é¢¨éšª:**
- xApps å¯ä»¥ç„¡é™åˆ¶åœ°è«‹æ±‚è³‡æº
- å–®ä¸€ xApp bug å¯èƒ½è€—ç›¡æ•´å€‹é›†ç¾¤è³‡æº
- ç„¡æ³•é˜²æ­¢ noisy neighbor å•é¡Œ

**å»ºè­°é…é¡è¨­å®š:**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: ricxapp-quota
  namespace: ricxapp
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    persistentvolumeclaims: "10"
    pods: "20"
```

---

## 2. æ•ˆèƒ½åŸºæº–æ¸¬è©¦ (Performance Baseline)

### 2.1 E2 è¨Šæ¯è™•ç†å»¶é² (E2 Message Processing Latency)

**ç›®æ¨™ (Targets):**
- E2 indication processing: < 10ms
- Control command latency: < 100ms
- RMR message throughput: > 10K msg/sec

**ç•¶å‰é…ç½®é™åˆ¶:**

1. **E2term è³‡æºé™åˆ¶**
   ```yaml
   resources:
     requests:
       cpu: 400m
       memory: 512Mi
     limits:
       cpu: 1000m  # å¯èƒ½é€ æˆ CPU throttling
       memory: 1Gi
   ```
   - 400m CPU request åœ¨é«˜è² è¼‰ä¸‹å¯èƒ½ä¸è¶³
   - 1000m limit æœƒå°è‡´ throttling

2. **RMR é…ç½®**
   ```yaml
   rmr:
     maxSize: 8192  # è¨Šæ¯å¤§å°é™åˆ¶
     numWorkers: 1  # å–®ä¸€ worker (traffic-steering)
   ```
   - å–®ä¸€ worker è™•ç†æ‰€æœ‰è¨Šæ¯é¡å‹
   - ç„¡æ³•å……åˆ†åˆ©ç”¨å¤šæ ¸å¿ƒ

3. **ç¼ºä¹è¨Šæ¯å„ªå…ˆç´š**
   - æ‰€æœ‰è¨Šæ¯é¡å‹ä½¿ç”¨ç›¸åŒçš„è™•ç†ä½‡åˆ—
   - Control messages å¯èƒ½è¢« indication messages é˜»å¡

### 2.2 xApp å•Ÿå‹•æ™‚é–“ (xApp Startup Time)

**ç›®æ¨™:** < 30 ç§’

**å¯¦éš›è§€å¯Ÿ (å¾ E2E æ¸¬è©¦):**
```
Pod å•Ÿå‹•æ™‚é–“åˆ†æï¼š
- kpimon: ~15s (åŒ…å« image pull)
- traffic-steering: ~12s
- rc-xapp: ~14s
- qoe-predictor: ~18s (ML model loading)
- federated-learning: ~25s (ML initialization)
```

**åˆ†æ:**
- âœ… æ‰€æœ‰ xApps éƒ½ç¬¦åˆ 30 ç§’ç›®æ¨™
- QoE predictor èˆ‡ FL xApp å•Ÿå‹•è¼ƒæ…¢ï¼ˆML model loadingï¼‰
- å¯é€é init containers é è¼‰æ¨¡å‹ä¾†å„ªåŒ–

### 2.3 è³‡æ–™åº«æ•ˆèƒ½ (Database Performance)

**DBaaS (Redis) é…ç½®:**

```yaml
dbaas:
  enableHighAvailability: false  # âš ï¸ æœªå•Ÿç”¨ HA
  redis:
    sa_config:
      appendonly: "no"  # âš ï¸ ç„¡æŒä¹…åŒ–
      save: ""          # âš ï¸ ç„¡ RDB å¿«ç…§
      maxmemory: "0"    # âš ï¸ ç„¡è¨˜æ†¶é«”é™åˆ¶
      loadmodule: "/usr/local/libexec/redismodule/libredismodule.so"
```

**åš´é‡å•é¡Œ:**

1. **ç„¡é«˜å¯ç”¨æ€§**
   - å–®ä¸€ Redis instance
   - ä»»ä½•æ•…éšœéƒ½æœƒå°è‡´æ‰€æœ‰ xApps å¤±å»ç‹€æ…‹

2. **ç„¡æŒä¹…åŒ–**
   - AOF: disabled
   - RDB: disabled
   - Pod restart æœƒéºå¤±æ‰€æœ‰è³‡æ–™

3. **ç„¡è¨˜æ†¶é«”é™åˆ¶**
   - `maxmemory: "0"` è¡¨ç¤ºç„¡é™åˆ¶
   - å¯èƒ½å°è‡´ OOM kill

4. **é€£ç·šç®¡ç†**
   - xApps ç›´æ¥é€£æ¥ Redisï¼Œç„¡é€£ç·šæ± 
   - å¯èƒ½å°è‡´é€£ç·šè€—ç›¡

**æ•ˆèƒ½å½±éŸ¿:**
- KPI è³‡æ–™å„²å­˜åœ¨ Redis (TTL 300 ç§’)
- ç„¡æŒä¹…åŒ–æ„å‘³è‘—é‡å•Ÿå¾Œéºå¤±æ‰€æœ‰æ­·å²è³‡æ–™
- å»ºè­°åŒæ™‚ä½¿ç”¨ InfluxDB é€²è¡Œé•·æœŸå„²å­˜

---

## 3. å¯æ“´å±•æ€§åˆ†æ (Scalability Analysis)

### 3.1 æ°´å¹³æ“´å±•èƒ½åŠ› (Horizontal Scaling)

**ç•¶å‰ç‹€æ…‹:**

```yaml
# æ‰€æœ‰çµ„ä»¶éƒ½æ˜¯å–®ä¸€ replica
e2term.replicas: 1
e2mgr.replicaCount: 1
submgr.replicaCount: 1
appmgr.replicaCount: 1

# xApps (from deployments)
kpimon.replicas: 1
traffic-steering.replicas: 1
```

**HPA é…ç½®:**

```yaml
autoscaling:
  enabled: false  # âš ï¸ æ‰€æœ‰çµ„ä»¶éƒ½æœªå•Ÿç”¨è‡ªå‹•æ“´å±•
```

**å•é¡Œåˆ†æ:**

1. **ç„¡è‡ªå‹•æ“´å±•æ©Ÿåˆ¶**
   - æµé‡å¢åŠ æ™‚ç„¡æ³•è‡ªå‹•æ“´å±•
   - éœ€è¦æ‰‹å‹•èª¿æ•´ replicas
   - ç„¡æ³•æ‡‰å°çªç™¼æµé‡

2. **ç‹€æ…‹ç®¡ç†å•é¡Œ**
   - E2term ä½¿ç”¨ SCTP é€£ç·šï¼Œé›£ä»¥è² è¼‰å‡è¡¡
   - Subscription Manager éœ€è¦ç‹€æ…‹åŒæ­¥æ©Ÿåˆ¶
   - xApps ç›´æ¥é€£æ¥ Redisï¼Œå¤šå‰¯æœ¬æœƒæœ‰ä¸€è‡´æ€§å•é¡Œ

3. **å»ºè­° HPA é…ç½®:**

```yaml
# E2mgr HPA (é©åˆæ°´å¹³æ“´å±•)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: e2mgr-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: e2mgr
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 180
```

### 3.2 å‚ç›´æ“´å±•é™åˆ¶ (Vertical Scaling Constraints)

**ç•¶å‰é™åˆ¶:**

1. **CPU Limits éä½**
   - E2term: 1000m (1 core)
   - åœ¨é«˜è¨Šæ¯é‡ä¸‹æœƒ CPU throttling
   - å»ºè­°è‡³å°‘ 2000m for production

2. **Memory Limits**
   - E2term: 1Gi
   - å°æ–¼è™•ç†å¤§é‡ E2 ç¯€é»å¯èƒ½ä¸è¶³
   - å»ºè­°æ ¹æ“š E2 ç¯€é»æ•¸é‡å‹•æ…‹èª¿æ•´ï¼š`memory = 512Mi + (num_e2_nodes * 50Mi)`

3. **VPA æœªå•Ÿç”¨**
   ```yaml
   verticalAutoscaler:
     enabled: false
   ```
   - ç„¡æ³•æ ¹æ“šå¯¦éš›ä½¿ç”¨é‡è‡ªå‹•èª¿æ•´è³‡æº

### 3.3 è² è¼‰å‡è¡¡ç­–ç•¥ (Load Balancing)

**ç•¶å‰ç‹€æ…‹:**

```yaml
service:
  sessionAffinity: None  # Prometheus, Alertmanager
```

**å•é¡Œ:**

1. **E2term è² è¼‰å‡è¡¡**
   - SCTP é€£ç·šéœ€è¦æœƒè©±ä¿æŒ
   - ç•¶å‰å–®ä¸€ instanceï¼Œç„¡è² è¼‰å‡è¡¡
   - å»ºè­°ä½¿ç”¨ä¸€è‡´æ€§é›œæ¹Š (Consistent Hashing) åˆ†é… E2 ç¯€é»

2. **xApp å…§éƒ¨è² è¼‰å‡è¡¡**
   - RMR è¨Šæ¯è·¯ç”±ç”± rtmgr ç®¡ç†
   - rtmgr ç•¶å‰ `enabled: false`ï¼ˆä½¿ç”¨éœæ…‹è·¯ç”±ï¼‰
   - é™åˆ¶äº†å‹•æ…‹è·¯ç”±å’Œè² è¼‰å‡è¡¡èƒ½åŠ›

3. **Service Mesh ç¼ºå¤±**
   ```yaml
   serviceMesh:
     enabled: false  # Linkerd æœªéƒ¨ç½²
   ```
   - ç„¡æ³•é€²è¡Œæ™ºæ…§è·¯ç”±
   - ç¼ºä¹ circuit breakerã€retry ç­‰éŸŒæ€§æ©Ÿåˆ¶
   - ç„¡æ³•å¯¦ç¾ canary deployment

---

## 4. ç“¶é ¸è­˜åˆ¥ (Bottleneck Identification)

### 4.1 E2 è¨Šæ¯è™•ç†è·¯å¾‘ (E2 Message Path)

**å®Œæ•´è·¯å¾‘åˆ†æ:**

```
E2 Node â†’ E2term (SCTP) â†’ E2mgr â†’ Submgr â†’ xApp
                â†“                              â†“
              RMR Messages â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ç“¶é ¸é»:**

1. **E2term â†’ RMR è½‰æ›** (ğŸ”´ Critical)
   - å–®ä¸€ worker thread
   - åŒæ­¥è™•ç† SCTP è¨Šæ¯
   - **å»ºè­°:** å¯¦ä½œè¨Šæ¯ä½‡åˆ—èˆ‡å¤š worker æ¶æ§‹

2. **Subscription Manager** (ğŸŸ¡ High)
   - è™•ç†æ‰€æœ‰è¨‚é–±è«‹æ±‚
   - å–®ä¸€ instanceï¼Œ100m CPU request
   - **å»ºè­°:** å¢åŠ  CPU request è‡³ 200mï¼Œå•Ÿç”¨å¤šå‰¯æœ¬

3. **RMR Routing** (ğŸŸ¡ High)
   - rtmgr disabledï¼Œä½¿ç”¨éœæ…‹è·¯ç”±
   - ç„¡æ³•å‹•æ…‹èª¿æ•´è·¯ç”±è¡¨
   - **å»ºè­°:** å•Ÿç”¨ rtmgr æˆ–å¯¦ä½œæ›´æ™ºæ…§çš„è·¯ç”±æ©Ÿåˆ¶

4. **xApp RMR Workers** (ğŸŸ¡ High)
   - Traffic-steering: 1 worker
   - è™•ç† indication + control + A1 policy
   - **å»ºè­°:** å¢åŠ è‡³ 2-4 workers

### 4.2 è³‡æ–™å„²å­˜è·¯å¾‘ (Data Storage Path)

**KPI è³‡æ–™æµ:**

```
E2 Indication â†’ kpimon â†’ Redis (TTL 300s)
                      â†“
                  InfluxDB (7 days retention)
```

**ç“¶é ¸é»:**

1. **Redis ç„¡é€£ç·šæ± ** (ğŸŸ¡ High)
   ```python
   # ç•¶å‰å¯¦ä½œï¼ˆæ¨æ¸¬ï¼‰
   redis_client = redis.Redis(host='redis-service.ricplt', port=6379)
   ```
   - æ¯å€‹è«‹æ±‚å»ºç«‹æ–°é€£ç·š
   - **å»ºè­°:** ä½¿ç”¨é€£ç·šæ± ï¼Œè¨­å®š max_connections

2. **InfluxDB å¯«å…¥æ•ˆèƒ½** (ğŸŸ¢ Medium)
   - ç•¶å‰æœªéƒ¨ç½² InfluxDB
   - é…ç½®ä¸­æœ‰è¨­å®šä½†æœªå¯¦éš›å®‰è£
   - **å»ºè­°:** éƒ¨ç½² InfluxDB ä¸¦å•Ÿç”¨æ‰¹æ¬¡å¯«å…¥

3. **Prometheus è³‡æ–™ç•™å­˜** (ğŸŸ¡ High)
   ```yaml
   retention: "15d"
   persistentVolume:
     enabled: false  # ä½¿ç”¨ emptyDir
   ```
   - Pod restart æœƒéºå¤±æ‰€æœ‰è³‡æ–™
   - **å»ºè­°:** å•Ÿç”¨ PVCï¼Œå¢åŠ  retention è‡³ 30d

### 4.3 ç›£æ§èˆ‡è¿½è¹¤è·¯å¾‘ (Monitoring & Tracing)

**ç•¶å‰ç‹€æ…‹:**

```yaml
# Prometheus æ¡é›†é…ç½®
scrape_interval: 1m      # âš ï¸ éé•·
scrape_timeout: 10s
evaluation_interval: 1m
```

**å•é¡Œ:**

1. **æ¡é›†é–“éš”éé•·**
   - 1 åˆ†é˜ç„¡æ³•æ•æ‰çŸ­æœŸæ•ˆèƒ½å•é¡Œ
   - E2 indication è™•ç†æ™‚é–“ç›®æ¨™ < 10ms
   - **å»ºè­°:** é—œéµ metrics æ¡é›†é–“éš”èª¿æ•´ç‚º 15s

2. **ç„¡åˆ†æ•£å¼è¿½è¹¤**
   - Jaeger adapter æœªéƒ¨ç½²
   - ç„¡æ³•è¿½è¹¤è·¨çµ„ä»¶çš„è«‹æ±‚å»¶é²
   - **å»ºè­°:** éƒ¨ç½² Jaeger ä¸¦æ•´åˆ OpenTelemetry

3. **Metrics æš´éœ²ä¸ä¸€è‡´**
   - éƒ¨åˆ† xApps æœ‰ `/metrics` endpoint
   - ä½†ç¼ºä¹çµ±ä¸€çš„ metrics æ ¼å¼è¦ç¯„
   - **å»ºè­°:** å®šç¾©æ¨™æº– metrics schema

---

## 5. å„ªåŒ–å»ºè­° (Optimization Recommendations)

### 5.1 çŸ­æœŸå„ªåŒ– (Quick Wins - 1-2 é€±)

#### 5.1.1 èª¿æ•´è³‡æºé…ç½®

**Priority: ğŸ”´ Critical**

```yaml
# ä¿®æ”¹ /home/thc1006/oran-ric-platform/platform/values/local.yaml

# 1. E2term - é—œéµè·¯å¾‘å„ªåŒ–
e2term:
  resources:
    requests:
      cpu: 800m        # å¾ 400m æå‡
      memory: 512Mi
    limits:
      cpu: 1500m       # å¾ 1000m æå‡ï¼Œé™ä½ throttling
      memory: 1Gi

# 2. Subscription Manager - æå‡è™•ç†èƒ½åŠ›
submgr:
  resources:
    requests:
      cpu: 200m        # å¾ 100m æå‡
      memory: 256Mi    # å¾ 128Mi æå‡
    limits:
      cpu: 400m        # å¾ 250m æå‡
      memory: 512Mi    # å¾ 256Mi æå‡

# 3. DBaaS - å•Ÿç”¨æŒä¹…åŒ–èˆ‡è¨˜æ†¶é«”é™åˆ¶
dbaas:
  redis:
    sa_config:
      appendonly: "yes"              # å•Ÿç”¨ AOF
      save: "900 1 300 10 60 10000"  # å•Ÿç”¨ RDB
      maxmemory: "768mb"             # è¨­å®šè¨˜æ†¶é«”é™åˆ¶
      maxmemory-policy: "allkeys-lru"  # ä½¿ç”¨ LRU é©…é€ç­–ç•¥
  resources:
    requests:
      cpu: 200m        # å¾ 100m æå‡
      memory: 512Mi    # å¾ 256Mi æå‡
    limits:
      cpu: 500m
      memory: 1Gi
  persistence:
    enabled: true      # å•Ÿç”¨æŒä¹…åŒ–
    size: 20Gi         # å¾ 10Gi æå‡
```

**é æœŸå½±éŸ¿:**
- E2 indication è™•ç†å»¶é²é™ä½ 30-40%
- Redis ç„¡è³‡æ–™éºå¤±é¢¨éšª
- é™ä½ CPU throttling ç™¼ç”Ÿç‡

#### 5.1.2 å„ªåŒ– Prometheus é…ç½®

**Priority: ğŸŸ¡ High**

```yaml
# ä¿®æ”¹ /home/thc1006/oran-ric-platform/config/prometheus-values.yaml

server:
  global:
    scrape_interval: 15s     # å¾ 1m ç¸®çŸ­
    scrape_timeout: 10s
    evaluation_interval: 15s  # å¾ 1m ç¸®çŸ­

  retention: "30d"           # å¾ 15d å»¶é•·

  persistentVolume:
    enabled: true            # å¾ false æ”¹ç‚º true
    size: 50Gi              # å¾ 8Gi å¢åŠ 
    storageClassName: local-path

  resources:
    requests:
      cpu: 1000m             # å¾ 500m æå‡
      memory: 2Gi            # å¾ 1Gi æå‡
    limits:
      cpu: 2000m             # å¾ 1000m æå‡
      memory: 4Gi            # å¾ 2Gi æå‡
```

**é æœŸå½±éŸ¿:**
- æ•æ‰æ›´ç´°ç·»çš„æ•ˆèƒ½å•é¡Œ
- è³‡æ–™ä¸æœƒå›  pod restart éºå¤±
- æ”¯æ´æ›´é•·æœŸçš„æ•ˆèƒ½è¶¨å‹¢åˆ†æ

#### 5.1.3 å¢åŠ  RMR Workers

**Priority: ğŸŸ¡ High**

```json
// ä¿®æ”¹ xApps é…ç½®

// kpimon: /home/thc1006/oran-ric-platform/xapps/kpimon-go-xapp/config/config.json
{
  "rmr": {
    "numWorkers": 4  // å¾ 2 æå‡è‡³ 4
  }
}

// traffic-steering: /home/thc1006/oran-ric-platform/xapps/traffic-steering/config/config.json
{
  "rmr": {
    "numWorkers": 3  // å¾ 1 æå‡è‡³ 3 (è™•ç† indication + control + A1)
  }
}
```

**é æœŸå½±éŸ¿:**
- RMR è¨Šæ¯è™•ç†ååé‡æå‡ 2-3 å€
- é™ä½è¨Šæ¯è™•ç†å»¶é²
- æ›´å¥½åœ°åˆ©ç”¨å¤šæ ¸å¿ƒ CPU

### 5.2 ä¸­æœŸå„ªåŒ– (1-2 å€‹æœˆ)

#### 5.2.1 å¯¦ä½œå¤šå±¤å¿«å–ç­–ç•¥ (Multi-tier Caching)

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: Application Cache (In-memory)             â”‚
â”‚ - Python dict/lru_cache                             â”‚
â”‚ - TTL: 10-30 seconds                                â”‚
â”‚ - Use case: å¸¸ç”¨ KPI æŸ¥è©¢ã€UE ç‹€æ…‹                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“ (cache miss)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: Distributed Cache (Redis)                 â”‚
â”‚ - Connection pooling                                â”‚
â”‚ - TTL: 5 minutes                                    â”‚
â”‚ - Use case: è·¨ xApp å…±äº«è³‡æ–™                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“ (cache miss)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: Time-series DB (InfluxDB)                 â”‚
â”‚ - Batch writes (1000 points/batch)                 â”‚
â”‚ - Retention: 7 days                                 â”‚
â”‚ - Use case: æ­·å² KPI æŸ¥è©¢ã€è¶¨å‹¢åˆ†æ                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å¯¦ä½œç¯„ä¾‹ (kpimon xApp):**

```python
# /home/thc1006/oran-ric-platform/xapps/kpimon-go-xapp/src/cache.py

from functools import lru_cache
import redis
from redis import ConnectionPool
import time

class MultiTierCache:
    def __init__(self):
        # Layer 1: In-memory LRU cache
        self.memory_cache = {}
        self.memory_ttl = {}

        # Layer 2: Redis with connection pool
        self.redis_pool = ConnectionPool(
            host='redis-service.ricplt',
            port=6379,
            max_connections=20,
            socket_keepalive=True
        )
        self.redis_client = redis.Redis(connection_pool=self.redis_pool)

    @lru_cache(maxsize=1000)
    def get_ue_kpi(self, ue_id: str, kpi_name: str):
        cache_key = f"ue:{ue_id}:kpi:{kpi_name}"

        # Layer 1: Check memory cache
        if cache_key in self.memory_cache:
            if time.time() < self.memory_ttl[cache_key]:
                return self.memory_cache[cache_key]

        # Layer 2: Check Redis
        value = self.redis_client.get(cache_key)
        if value:
            # Populate memory cache
            self.memory_cache[cache_key] = value
            self.memory_ttl[cache_key] = time.time() + 30
            return value

        # Layer 3: Fetch from InfluxDB (not shown)
        return None
```

**é æœŸå½±éŸ¿:**
- KPI æŸ¥è©¢å»¶é²é™ä½ 80-90%
- Redis è² è¼‰é™ä½ 50-70%
- æ”¯æ´æ›´é«˜çš„æŸ¥è©¢ååé‡

#### 5.2.2 å•Ÿç”¨ Redis HA èˆ‡ Sentinel

**Priority: ğŸ”´ Critical**

```yaml
# ä¿®æ”¹ dbaas é…ç½®
dbaas:
  enableHighAvailability: true  # å•Ÿç”¨ HA
  haReplicas: 3                 # 3 å€‹ Redis replicas

  sentinel:
    quorum: 2                   # 2/3 é”æˆå…±è­˜
    config:
      down-after-milliseconds: 5000
      failover-timeout: 60000
      parallel-syncs: 1

  redis:
    ha_config:
      appendonly: "yes"
      save: "900 1 300 10"
      min-slaves-to-write: 1    # è‡³å°‘ 1 å€‹ slave ç¢ºèªå¯«å…¥
      min-slaves-max-lag: 5
      maxmemory: "768mb"
      maxmemory-policy: "allkeys-lru"
```

**é æœŸå½±éŸ¿:**
- ç„¡å–®é»æ•…éšœ
- è‡ªå‹•æ•…éšœè½‰ç§» (< 60 ç§’)
- è³‡æ–™æŒä¹…åŒ–ä¿è­‰

#### 5.2.3 éƒ¨ç½² Jaeger åˆ†æ•£å¼è¿½è¹¤

**Priority: ğŸŸ¡ High**

```bash
# éƒ¨ç½² Jaeger
helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
helm install jaeger jaegertracing/jaeger \
  --namespace ricobs \
  --set provisionDataStore.cassandra=false \
  --set storage.type=memory \
  --set collector.resources.limits.cpu=500m \
  --set collector.resources.limits.memory=512Mi
```

**æ•´åˆ xApps with OpenTelemetry:**

```python
# /home/thc1006/oran-ric-platform/xapps/kpimon-go-xapp/src/tracing.py

from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

def setup_tracing(service_name: str):
    trace.set_tracer_provider(TracerProvider())

    jaeger_exporter = JaegerExporter(
        agent_host_name="jaeger-agent.ricobs",
        agent_port=6831,
    )

    trace.get_tracer_provider().add_span_processor(
        BatchSpanProcessor(jaeger_exporter)
    )

    return trace.get_tracer(service_name)

# ä½¿ç”¨ç¯„ä¾‹
tracer = setup_tracing("kpimon-xapp")

@tracer.start_as_current_span("process_e2_indication")
def process_indication(message):
    with tracer.start_as_current_span("parse_asn1"):
        # ASN.1 parsing
        pass

    with tracer.start_as_current_span("extract_kpis"):
        # KPI extraction
        pass

    with tracer.start_as_current_span("store_redis"):
        # Store to Redis
        pass
```

**é æœŸå½±éŸ¿:**
- å¯è¦–åŒ–å®Œæ•´çš„è«‹æ±‚è·¯å¾‘
- è­˜åˆ¥å»¶é²ç“¶é ¸ (å“ªå€‹çµ„ä»¶æœ€æ…¢)
- æ”¯æ´ root cause analysis

### 5.3 é•·æœŸå„ªåŒ– (3-6 å€‹æœˆ)

#### 5.3.1 å¯¦ä½œ Service Mesh (Linkerd)

**Priority: ğŸŸ¡ High**

**Benefits:**
- Automatic mTLS between services
- Circuit breaking & retries
- Traffic splitting (canary deployments)
- Rich observability metrics

**éƒ¨ç½²æ­¥é©Ÿ:**

```bash
# 1. å®‰è£ Linkerd CLI
curl -sL https://run.linkerd.io/install | sh
export PATH=$PATH:$HOME/.linkerd2/bin

# 2. å®‰è£ Linkerd control plane
linkerd install --crds | kubectl apply -f -
linkerd install | kubectl apply -f -

# 3. Inject Linkerd proxy åˆ° RIC namespaces
kubectl get deploy -n ricplt -o yaml | linkerd inject - | kubectl apply -f -
kubectl get deploy -n ricxapp -o yaml | linkerd inject - | kubectl apply -f -

# 4. å®‰è£ Linkerd Viz (observability)
linkerd viz install | kubectl apply -f -
```

**é…ç½® Traffic Policy:**

```yaml
apiVersion: policy.linkerd.io/v1beta1
kind: Server
metadata:
  name: e2term-sctp
  namespace: ricplt
spec:
  podSelector:
    matchLabels:
      app: e2term
  port: 36422
  proxyProtocol: opaque  # SCTP traffic
---
apiVersion: policy.linkerd.io/v1alpha1
kind: HTTPRoute
metadata:
  name: xapp-routing
  namespace: ricxapp
spec:
  parentRefs:
    - name: kpimon
      kind: Service
  rules:
    - matches:
      - path:
          type: PathPrefix
          value: /metrics
      backendRefs:
        - name: kpimon
          port: 8080
```

**é æœŸå½±éŸ¿:**
- Service-to-service latency é™ä½ 10-15%
- Automatic retry æ¸›å°‘æš«æ™‚æ€§éŒ¯èª¤
- æ›´è±å¯Œçš„ service-level metrics

#### 5.3.2 å¯¦ä½œ E2 è¨Šæ¯æ‰¹æ¬¡è™•ç†

**Priority: ğŸ”´ Critical (for high-load scenarios)**

**Current State:**
```python
# æ¯å€‹ E2 indication å–®ç¨è™•ç†
def handle_indication(message):
    kpi = extract_kpi(message)
    store_to_redis(kpi)  # æ¯æ¬¡å–®ç¨å¯«å…¥
```

**Optimized with Batching:**

```python
import asyncio
from collections import defaultdict

class E2IndicationBatcher:
    def __init__(self, batch_size=100, batch_timeout=0.5):
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.buffer = []
        self.lock = asyncio.Lock()

    async def add_indication(self, message):
        async with self.lock:
            self.buffer.append(message)

            if len(self.buffer) >= self.batch_size:
                await self.flush()

    async def flush(self):
        if not self.buffer:
            return

        batch = self.buffer[:]
        self.buffer.clear()

        # æ‰¹æ¬¡è™•ç†
        kpis = [extract_kpi(msg) for msg in batch]

        # æ‰¹æ¬¡å¯«å…¥ Redis (pipeline)
        pipe = redis_client.pipeline()
        for kpi in kpis:
            pipe.setex(f"kpi:{kpi.ue_id}", 300, json.dumps(kpi))
        await pipe.execute()

        # æ‰¹æ¬¡å¯«å…¥ InfluxDB
        await influx_client.write_points(kpis)

    async def start_timer(self):
        while True:
            await asyncio.sleep(self.batch_timeout)
            async with self.lock:
                await self.flush()
```

**é æœŸå½±éŸ¿:**
- Redis å¯«å…¥ååé‡æå‡ 10-20 å€
- InfluxDB å¯«å…¥ååé‡æå‡ 50-100 å€
- CPU usage é™ä½ 30-40% (æ¸›å°‘ syscall overhead)

#### 5.3.3 å„ªåŒ– E2term æ¶æ§‹

**Priority: ğŸ”´ Critical**

**å•é¡Œ:** å–®ä¸€ E2term instance è™•ç†æ‰€æœ‰ E2 ç¯€é»

**è§£æ±ºæ–¹æ¡ˆ:** Sharded E2term with Consistent Hashing

```yaml
# éƒ¨ç½² 3 å€‹ E2term instances
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: e2term
  namespace: ricplt
spec:
  serviceName: e2term
  replicas: 3
  selector:
    matchLabels:
      app: e2term
  template:
    metadata:
      labels:
        app: e2term
    spec:
      containers:
      - name: e2term
        image: nexus3.o-ran-sc.org:10002/o-ran-sc/ric-plt-e2:5.5.0
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SHARD_ID
          value: "$(POD_NAME | sed 's/e2term-//')"
        resources:
          requests:
            cpu: 800m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi
---
# Headless service for direct pod access
apiVersion: v1
kind: Service
metadata:
  name: e2term-headless
  namespace: ricplt
spec:
  clusterIP: None
  selector:
    app: e2term
  ports:
  - name: sctp
    port: 36422
    protocol: SCTP
```

**E2 Node Assignment Logic:**

```python
import hashlib

def get_e2term_shard(e2_node_id: str, num_shards: int = 3) -> int:
    """ä½¿ç”¨ä¸€è‡´æ€§é›œæ¹Šåˆ†é… E2 node åˆ° E2term shard"""
    hash_value = int(hashlib.md5(e2_node_id.encode()).hexdigest(), 16)
    return hash_value % num_shards

# Example usage
e2_node_id = "gnb-001"
shard_id = get_e2term_shard(e2_node_id)
e2term_host = f"e2term-{shard_id}.e2term-headless.ricplt.svc.cluster.local"
```

**é æœŸå½±éŸ¿:**
- E2 è¨Šæ¯è™•ç†èƒ½åŠ›æå‡ 3 å€
- å–®ä¸€ shard æ•…éšœä¸å½±éŸ¿å…¶ä»– E2 nodes
- æ”¯æ´ 100+ E2 nodes

---

## 6. ç›£æ§èˆ‡å‘Šè­¦å»ºè­° (Monitoring & Alerting)

### 6.1 é—œéµæ•ˆèƒ½æŒ‡æ¨™ (Key Performance Indicators)

**E2 Interface Metrics:**

```yaml
# Prometheus recording rules
groups:
- name: e2_performance
  interval: 15s
  rules:
  - record: e2:indication_processing_latency:p99
    expr: histogram_quantile(0.99, rate(e2_indication_duration_seconds_bucket[5m]))

  - record: e2:indication_throughput:rate5m
    expr: rate(e2_indication_total[5m])

  - record: e2:control_req_latency:p99
    expr: histogram_quantile(0.99, rate(e2_control_req_duration_seconds_bucket[5m]))

  - record: e2:subscription_success_rate
    expr: rate(e2_subscription_success_total[5m]) / rate(e2_subscription_total[5m])
```

**Alerting Rules:**

```yaml
groups:
- name: e2_alerts
  rules:
  - alert: E2IndicationProcessingSlowWarning
    expr: e2:indication_processing_latency:p99 > 0.010  # > 10ms
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "E2 indication processing is slow"
      description: "P99 latency is {{ $value }}s (target: < 10ms)"

  - alert: E2IndicationProcessingSlowCritical
    expr: e2:indication_processing_latency:p99 > 0.050  # > 50ms
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "E2 indication processing is critically slow"
      description: "P99 latency is {{ $value }}s (target: < 10ms)"

  - alert: E2ControlLatencyHigh
    expr: e2:control_req_latency:p99 > 0.100  # > 100ms
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "E2 control command latency is high"
      description: "P99 latency is {{ $value }}s (target: < 100ms)"

  - alert: E2SubscriptionFailureRate
    expr: e2:subscription_success_rate < 0.95  # < 95%
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "E2 subscription failure rate is high"
      description: "Success rate is {{ $value }} (target: > 95%)"
```

**Resource Utilization Alerts:**

```yaml
- alert: PodCPUThrottling
  expr: rate(container_cpu_cfs_throttled_seconds_total{namespace=~"ricplt|ricxapp"}[5m]) > 0.1
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "Pod {{ $labels.pod }} is experiencing CPU throttling"
    description: "Throttling rate: {{ $value }}/s"

- alert: PodMemoryPressure
  expr: container_memory_working_set_bytes{namespace=~"ricplt|ricxapp"} / container_spec_memory_limit_bytes > 0.9
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "Pod {{ $labels.pod }} memory usage is high"
    description: "Memory usage: {{ $value | humanizePercentage }}"
```

### 6.2 Grafana Dashboards

**å»ºè­°å»ºç«‹ä»¥ä¸‹ dashboards:**

1. **O-RAN RIC Overview Dashboard**
   - æ•´é«”å¥åº·ç‹€æ…‹
   - E2 nodes é€£æ¥æ•¸é‡
   - xApps é‹è¡Œç‹€æ…‹
   - è³‡æºä½¿ç”¨ç‡æ‘˜è¦

2. **E2 Interface Performance Dashboard**
   - E2 indication è™•ç†å»¶é² (P50, P95, P99)
   - E2 control å‘½ä»¤å»¶é²
   - Subscription æˆåŠŸç‡
   - è¨Šæ¯ååé‡ (msg/sec)

3. **xApp Performance Dashboard**
   - æ¯å€‹ xApp çš„ CPU/Memory ä½¿ç”¨ç‡
   - RMR è¨Šæ¯è™•ç†å»¶é²
   - Redis æ“ä½œå»¶é²
   - éŒ¯èª¤ç‡èˆ‡é‡è©¦æ¬¡æ•¸

4. **Resource Utilization Dashboard**
   - Node-level metrics
   - Namespace quotas vs usage
   - Pod CPU throttling events
   - OOM kills

**Dashboard åŒ¯å…¥è…³æœ¬:**

```bash
# /home/thc1006/oran-ric-platform/scripts/import-dashboards.sh

#!/bin/bash

GRAFANA_URL="http://localhost:3000"
GRAFANA_USER="admin"
GRAFANA_PASSWORD=$(kubectl get secret -n ricplt oran-grafana -o jsonpath="{.data.admin-password}" | base64 -d)

# Import dashboards
for dashboard in ./config/grafana-dashboards/*.json; do
  echo "Importing dashboard: $dashboard"
  curl -X POST \
    -H "Content-Type: application/json" \
    -u "$GRAFANA_USER:$GRAFANA_PASSWORD" \
    -d @"$dashboard" \
    "$GRAFANA_URL/api/dashboards/db"
done
```

---

## 7. å®¹é‡è¦åŠƒ (Capacity Planning)

### 7.1 E2 Nodes æ“´å±•ä¼°ç®—

**å‡è¨­:**
- æ¯å€‹ E2 node ç”¢ç”Ÿ 100 indications/sec
- æ¯å€‹ indication å¹³å‡å¤§å° 2KB
- ç›®æ¨™æ”¯æ´ 50 å€‹ E2 nodes

**è¨ˆç®—:**

```
Total indications/sec = 50 * 100 = 5,000 msg/sec
Total data rate = 5,000 * 2KB = 10 MB/sec

E2term è³‡æºéœ€æ±‚:
- CPU: æ¯ 1000 msg/sec éœ€è¦ ~200m CPU
  éœ€æ±‚: 5 * 200m = 1000m = 1 core
  å»ºè­° (with headroom): 2 cores

- Memory: æ¯ 1000 msg/sec éœ€è¦ ~100Mi
  éœ€æ±‚: 5 * 100Mi = 500Mi
  å»ºè­° (with headroom): 1Gi

RMR buffer:
- è¨Šæ¯ä½‡åˆ—æ·±åº¦: 1000 messages
- Buffer size: 1000 * 8KB = 8MB (per xApp)
```

**å»ºè­°é…ç½® (50 E2 nodes):**

```yaml
e2term:
  replicas: 2  # HA + load distribution
  resources:
    requests:
      cpu: 1500m
      memory: 1.5Gi
    limits:
      cpu: 3000m
      memory: 3Gi

kpimon:
  replicas: 2
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 2000m
      memory: 2Gi
```

### 7.2 Storage æ“´å±•ä¼°ç®—

**KPI è³‡æ–™é‡ä¼°ç®—:**

```
å‡è¨­:
- 50 E2 nodes
- æ¯å€‹ node å¹³å‡ 20 UEs
- æ¯å€‹ UE 20 å€‹ KPIs
- æ¯å€‹ KPI æ¯ç§’æ›´æ–° 1 æ¬¡
- æ¯å€‹ KPI è³‡æ–™é» ~100 bytes

Redis (çŸ­æœŸå¿«å–, TTL 5 min):
= 50 nodes * 20 UEs * 20 KPIs * 100 bytes
= 2 MB (å¿«ç…§è³‡æ–™)
å»ºè­° Redis memory: 1GB (500x headroom for overhead)

InfluxDB (7 days retention):
æ¯ç§’è³‡æ–™é» = 50 * 20 * 20 = 20,000 points/sec
æ¯å¤©è³‡æ–™é‡ = 20,000 * 86400 * 100 bytes = 172 GB/day
7 å¤©è³‡æ–™é‡ = 172 * 7 = 1.2 TB
å»ºè­° InfluxDB storage: 2 TB (with compression)

Prometheus (30 days retention):
å‡è¨­ 100 å€‹ metrics per service, 10 services
æ¯ç§’ series = 100 * 10 = 1000 series
æ¯å¤©è³‡æ–™é‡ = 1000 * 86400 * 8 bytes = 691 MB/day
30 å¤©è³‡æ–™é‡ = 691 * 30 = 20.7 GB
å»ºè­° Prometheus storage: 50 GB (with headroom)
```

---

## 8. æ¸¬è©¦èˆ‡é©—è­‰è¨ˆç•« (Testing & Validation Plan)

### 8.1 æ•ˆèƒ½æ¸¬è©¦å ´æ™¯

#### Scenario 1: Baseline Performance Test

**ç›®æ¨™:** å»ºç«‹æ•ˆèƒ½åŸºæº–ç·š

```yaml
Test Configuration:
  - E2 nodes: 5
  - Indications/sec per node: 10
  - Test duration: 1 hour
  - Metrics to collect:
    - E2 indication latency (P50, P95, P99)
    - CPU/Memory usage
    - RMR message queue depth
```

**Acceptance Criteria:**
- âœ… P99 latency < 10ms
- âœ… CPU usage < 50%
- âœ… No error messages

#### Scenario 2: Load Test

**ç›®æ¨™:** é©—è­‰ç³»çµ±åœ¨é«˜è² è¼‰ä¸‹çš„è¡¨ç¾

```yaml
Test Configuration:
  - E2 nodes: 20
  - Indications/sec per node: 50
  - Test duration: 30 minutes
  - Ramp-up: 5 minutes
```

**Acceptance Criteria:**
- âœ… P99 latency < 50ms
- âœ… No message loss
- âœ… CPU usage < 80%

#### Scenario 3: Stress Test

**ç›®æ¨™:** æ‰¾å‡ºç³»çµ±æ¥µé™

```yaml
Test Configuration:
  - E2 nodes: 50
  - Indications/sec per node: 100
  - Test duration: 15 minutes
  - Expected behavior: Graceful degradation
```

**Acceptance Criteria:**
- âœ… System remains stable (no crashes)
- âœ… Error rate < 5%
- âœ… Latency < 200ms (degraded but acceptable)

#### Scenario 4: Failover Test

**ç›®æ¨™:** é©—è­‰é«˜å¯ç”¨æ€§

```yaml
Test Configuration:
  - å•Ÿç”¨ Redis HA (3 replicas)
  - æ¸¬è©¦ä¸­ kill Redis master
  - æ¸¬è©¦ä¸­ kill E2term pod

Expected Behavior:
  - Redis: Automatic failover < 60s
  - E2term: Pod restart < 30s, E2 nodes reconnect
```

### 8.2 æ¸¬è©¦å·¥å…·

**1. E2 Simulator Enhancement**

```bash
# /home/thc1006/oran-ric-platform/simulator/e2-simulator/load-test.py

import asyncio
import time
from typing import List
import statistics

class E2LoadTester:
    def __init__(self, num_nodes: int, msg_rate: int):
        self.num_nodes = num_nodes
        self.msg_rate = msg_rate
        self.latencies: List[float] = []

    async def simulate_node(self, node_id: int):
        """Simulate single E2 node sending indications"""
        interval = 1.0 / self.msg_rate

        while True:
            start = time.time()

            # Send E2 indication
            await self.send_indication(node_id)

            # Measure latency
            latency = time.time() - start
            self.latencies.append(latency)

            # Rate limiting
            await asyncio.sleep(interval)

    async def send_indication(self, node_id: int):
        # Simulate E2 indication message
        pass

    def report_metrics(self):
        """Print latency statistics"""
        if not self.latencies:
            return

        print(f"Total messages: {len(self.latencies)}")
        print(f"P50 latency: {statistics.quantiles(self.latencies, n=2)[0]:.3f}s")
        print(f"P95 latency: {statistics.quantiles(self.latencies, n=20)[18]:.3f}s")
        print(f"P99 latency: {statistics.quantiles(self.latencies, n=100)[98]:.3f}s")
        print(f"Max latency: {max(self.latencies):.3f}s")

# Run load test
async def main():
    tester = E2LoadTester(num_nodes=20, msg_rate=50)

    # Start all simulated nodes
    tasks = [tester.simulate_node(i) for i in range(tester.num_nodes)]

    # Run for 30 minutes
    await asyncio.wait_for(
        asyncio.gather(*tasks),
        timeout=1800
    )

    tester.report_metrics()

if __name__ == "__main__":
    asyncio.run(main())
```

**2. Prometheus Query for Analysis**

```promql
# E2 indication latency over time
rate(e2_indication_duration_seconds_sum[5m]) /
rate(e2_indication_duration_seconds_count[5m])

# CPU throttling events
rate(container_cpu_cfs_throttled_seconds_total{namespace="ricplt"}[5m])

# Memory pressure
container_memory_working_set_bytes{namespace="ricplt"} /
container_spec_memory_limit_bytes * 100
```

---

## 9. æˆæœ¬æ•ˆç›Šåˆ†æ (Cost-Benefit Analysis)

### 9.1 å„ªåŒ–æŠ•è³‡å›å ± (ROI)

| å„ªåŒ–é …ç›® | å¯¦æ–½æˆæœ¬ (äººå¤©) | é æœŸæ•ˆèƒ½æå‡ | å„ªå…ˆç´š | ROI |
|---------|---------------|------------|-------|-----|
| èª¿æ•´è³‡æºé…ç½® | 2 | 30-40% latency reduction | ğŸ”´ Critical | â­â­â­â­â­ |
| å¢åŠ  RMR workers | 1 | 2-3x throughput | ğŸŸ¡ High | â­â­â­â­â­ |
| å„ªåŒ– Prometheus | 2 | Better observability | ğŸŸ¡ High | â­â­â­â­ |
| Redis HA | 5 | Zero downtime | ğŸ”´ Critical | â­â­â­â­ |
| Multi-tier cache | 10 | 80-90% query latency reduction | ğŸŸ¡ High | â­â­â­â­ |
| Jaeger tracing | 8 | Debugging efficiency | ğŸŸ¡ High | â­â­â­ |
| Service Mesh | 15 | 10-15% latency reduction | ğŸŸ¢ Medium | â­â­â­ |
| E2 batching | 12 | 10-20x write throughput | ğŸ”´ Critical | â­â­â­â­â­ |
| E2term sharding | 20 | 3x capacity | ğŸŸ¢ Medium | â­â­â­ |

**å»ºè­°å¯¦æ–½é †åº:**
1. Week 1-2: èª¿æ•´è³‡æºé…ç½® + å¢åŠ  RMR workers (Quick wins)
2. Week 3-4: å„ªåŒ– Prometheus + Redis HA
3. Month 2: Multi-tier cache + Jaeger tracing
4. Month 3-4: E2 batching
5. Month 5-6: Service Mesh + E2term sharding

### 9.2 ç¡¬é«”æˆæœ¬ä¼°ç®—

**ç•¶å‰é…ç½® (å–®ç¯€é»):**
```
CPU: 32 cores (3.99% utilized = 1.28 cores used)
Memory: 48 GB (14.98% utilized = 7.2 GB used)
ä¼°è¨ˆæˆæœ¬: $500/month (AWS c5.9xlarge equivalent)

å¯¦éš›éœ€æ±‚: < $100/month (å¯ä½¿ç”¨ c5.xlarge)
æµªè²»: $400/month
```

**å„ªåŒ–å¾Œé…ç½® (50 E2 nodes, 20 xApps):**
```
æ§åˆ¶å¹³é¢ç¯€é» (k3s master):
- c5.2xlarge (8 vCPU, 16 GB RAM): $200/month

å·¥ä½œç¯€é» 1 (Platform components):
- c5.2xlarge (8 vCPU, 16 GB RAM): $200/month

å·¥ä½œç¯€é» 2 (xApps):
- c5.4xlarge (16 vCPU, 32 GB RAM): $400/month

å·¥ä½œç¯€é» 3 (Observability):
- r5.xlarge (4 vCPU, 32 GB RAM): $250/month

ç¸½æˆæœ¬: $1,050/month
ROI: æ”¯æ´ 10x è² è¼‰ï¼Œæˆæœ¬åƒ…å¢åŠ  2x
```

---

## 10. çµè«–èˆ‡è¡Œå‹•è¨ˆç•« (Conclusion & Action Plan)

### 10.1 é—œéµçµè«–

1. **è³‡æºé…ç½®éœ€è¦é‡æ–°è©•ä¼°**
   - ç•¶å‰é…ç½®éæ–¼ä¿å®ˆï¼ˆè³‡æºä½¿ç”¨ç‡ < 15%ï¼‰
   - åŒæ™‚å­˜åœ¨æ½›åœ¨ç“¶é ¸ï¼ˆE2term CPU limit, submgr resourcesï¼‰
   - éœ€è¦æ ¹æ“šå¯¦éš›è² è¼‰é€²è¡Œèª¿æ•´

2. **ç¼ºä¹é«˜å¯ç”¨æ€§ä¿è­‰**
   - Redis å–®é»æ•…éšœé¢¨éšª
   - ç„¡è‡ªå‹•æ•…éšœè½‰ç§»æ©Ÿåˆ¶
   - è³‡æ–™æŒä¹…åŒ–æœªå•Ÿç”¨

3. **å¯è§€æ¸¬æ€§ä¸è¶³**
   - Metrics æ¡é›†é–“éš”éé•·ï¼ˆ1 åˆ†é˜ï¼‰
   - ç„¡åˆ†æ•£å¼è¿½è¹¤
   - ç¼ºä¹ç«¯åˆ°ç«¯çš„æ•ˆèƒ½å¯è¦–åŒ–

4. **æ“´å±•æ€§å—é™**
   - ç„¡ HPA/VPA
   - å–®ä¸€ E2term instance
   - ç„¡ Service Mesh

### 10.2 30 å¤©è¡Œå‹•è¨ˆç•«

**Week 1: Quick Wins**
- âœ… Day 1-2: èª¿æ•´ E2term, submgr, DBaaS è³‡æºé…ç½®
- âœ… Day 3: å¢åŠ  xApps RMR workers
- âœ… Day 4-5: å„ªåŒ– Prometheus é…ç½®ï¼ˆæ¡é›†é–“éš”ã€retentionã€PVCï¼‰

**Week 2: High Availability**
- âœ… Day 6-8: å•Ÿç”¨ Redis HA èˆ‡ Sentinel
- âœ… Day 9: é…ç½® Redis æŒä¹…åŒ–ï¼ˆAOF + RDBï¼‰
- âœ… Day 10: æ¸¬è©¦ Redis failover

**Week 3: Observability**
- âœ… Day 11-13: éƒ¨ç½² Jaeger
- âœ… Day 14-15: æ•´åˆ xApps with OpenTelemetry
- âœ… Day 16-17: å»ºç«‹ Grafana dashboards

**Week 4: Testing & Validation**
- âœ… Day 18-20: E2 Simulator è² è¼‰æ¸¬è©¦
- âœ… Day 21-22: æ•ˆèƒ½åŸºæº–æ¸¬è©¦
- âœ… Day 23-24: å£“åŠ›æ¸¬è©¦èˆ‡èª¿å„ª
- âœ… Day 25: æ–‡æª”æ›´æ–°

**Week 5+: Advanced Optimization**
- Day 26-30: å¯¦ä½œ multi-tier caching
- Month 2: E2 è¨Šæ¯æ‰¹æ¬¡è™•ç†
- Month 3: Service Mesh deployment
- Month 4-6: E2term sharding

### 10.3 æˆåŠŸæŒ‡æ¨™ (Success Metrics)

**æ•ˆèƒ½ç›®æ¨™:**
- âœ… E2 indication P99 latency < 10ms
- âœ… Control command latency < 100ms
- âœ… RMR throughput > 10K msg/sec
- âœ… xApp startup time < 30s

**å¯é æ€§ç›®æ¨™:**
- âœ… Redis uptime > 99.9%
- âœ… Automatic failover < 60s
- âœ… Zero data loss on pod restart

**å¯è§€æ¸¬æ€§ç›®æ¨™:**
- âœ… All components expose Prometheus metrics
- âœ… End-to-end tracing coverage > 90%
- âœ… Alert response time < 5 minutes

**æˆæœ¬ç›®æ¨™:**
- âœ… æ”¯æ´ 50 E2 nodes èˆ‡ 20 xApps
- âœ… ç¡¬é«”æˆæœ¬ < $1,500/month
- âœ… è³‡æºä½¿ç”¨æ•ˆç‡ > 60%

---

## é™„éŒ„ A: é…ç½®æª”æ¡ˆæ¸…å–® (Configuration Files)

éœ€è¦ä¿®æ”¹çš„é…ç½®æª”æ¡ˆï¼š

1. `/home/thc1006/oran-ric-platform/platform/values/local.yaml` - Platform resources
2. `/home/thc1006/oran-ric-platform/config/prometheus-values.yaml` - Monitoring
3. `/home/thc1006/oran-ric-platform/xapps/*/config/config.json` - xApp configs
4. `/home/thc1006/oran-ric-platform/ric-dep/helm/dbaas/values.yaml` - Redis HA
5. æ–°å¢: `/home/thc1006/oran-ric-platform/config/resource-quotas.yaml` - Quotas
6. æ–°å¢: `/home/thc1006/oran-ric-platform/config/hpa-policies.yaml` - Autoscaling
7. æ–°å¢: `/home/thc1006/oran-ric-platform/config/prometheus-alerts.yaml` - Alerting

---

## é™„éŒ„ B: åƒè€ƒè³‡æ–™ (References)

1. O-RAN Alliance specifications:
   - O-RAN.WG3.E2AP-v03.00: E2 Application Protocol
   - O-RAN.WG3.E2SM-KPM-v03.00: KPM Service Model
   - O-RAN.WG5.C.1-v07.00: Control Loop Specification

2. Kubernetes best practices:
   - Resource management: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
   - QoS classes: https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/
   - HPA: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/

3. Redis performance:
   - Redis Sentinel: https://redis.io/docs/manual/sentinel/
   - Redis persistence: https://redis.io/docs/manual/persistence/

4. Observability:
   - Prometheus best practices: https://prometheus.io/docs/practices/naming/
   - OpenTelemetry: https://opentelemetry.io/docs/
   - Jaeger architecture: https://www.jaegertracing.io/docs/architecture/

---

**å ±å‘ŠçµæŸ**

**ä½œè€…:** è”¡ç§€å‰ (thc1006)
**æ—¥æœŸ:** 2025-11-17
**ç‰ˆæœ¬:** 1.0
